function center = approxCircleCenter(points, x0, maxIter)
    % points : ensemble des points (N x 2) à partir desquels approximer le centre
    % x0 : estimation initiale du centre [cx; cy]
    % maxIter : nombre maximum d'itérations

    f = @(c) costFunction(c, points);
    grad_f = @(c) costGradient(c, points);

    % Utilisation de la méthode quasi-Newton pour minimiser f
    center = quasiNewtonBFGS(f, grad_f, x0, maxIter);
end


function center = quasiNewtonBFGS(f, grad_f, x0, maxIter)
    % f : fonction coût
    % grad_f : gradient de la fonction coût
    % x0 : estimation initiale du centre
    % maxIter : nombre maximum d'itérations

    tol = 1e-6;  % Tolérance pour l'arrêt
    n = length(x0);
    H = eye(n);  % Initialisation de l'approximation de l'inverse de la hessienne
    x = x0;

    for k = 1:maxIter
        grad = grad_f(x);  % Calcul du gradient
        if norm(grad) < tol
            break;
        end

        % Calcul de la direction de descente
        d = -H * grad;

        % Recherche linéaire avec la méthode de Fletcher-Lemaréchal
        alpha = fletcherLemarechal(f, grad_f, x, d, 1e-4, 0.9, 0.1, 0.5);

        % Mise à jour de la position
        x_next = x + alpha * d;

        % Mise à jour des variables pour BFGS
        s = x_next - x;
        y = grad_f(x_next) - grad;
        
        % Mise à jour de l'approximation de l'inverse de la hessienne H avec BFGS
        if s' * y > 0  % Condition pour garantir que H reste définie positive
            rho = 1 / (y' * s);
            H = (eye(n) - rho * (s * y')) * H * (eye(n) - rho * (y * s')) + rho * (s * s');
        end

        % Mise à jour pour l'itération suivante
        x = x_next;
    end
    
    center = x;
end

-----------------------------------------------------------------------------------


function alpha = fletcherLemarechal(f, grad_f, x, d, c1, c2, alpha_init, beta)
    % Recherche linéaire selon les conditions de Wolfe (Fletcher-Lemaréchal)
    % f       : fonction coût
    % grad_f  : gradient de la fonction coût
    % x       : point de départ
    % d       : direction de descente
    % c1, c2  : paramètres des conditions de Wolfe
    % alpha_init : valeur initiale pour alpha
    % beta    : facteur de réduction

    alpha = alpha_init;
    maxIter = 20;

    for i = 1:maxIter
        % Évaluer la fonction coût et son gradient
        f_x_alpha = f(x(1) + alpha * d(1), x(2) + alpha * d(2));
        grad_x = grad_f(x(1), x(2));
        f_x = f(x(1), x(2));
        
        % Condition de Wolfe 1 : Sufficient decrease
        if f_x_alpha > f_x + c1 * alpha * (grad_x' * d)
            alpha = alpha * beta;  % Réduction de alpha
        else
            % Condition de Wolfe 2 : Curvature condition
            grad_f_x_alpha = grad_f(x(1) + alpha * d(1), x(2) + alpha * d(2));
            if grad_f_x_alpha' * d < c2 * (grad_x' * d)
                alpha = alpha / beta;  % Augmentation de alpha
            else
                break;  % Les deux conditions de Wolfe sont satisfaites
            end
        end
    end
end    
